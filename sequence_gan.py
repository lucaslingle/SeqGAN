import os
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import datetime
from collections import OrderedDict
from dataloader import Gen_Dataloader, Dis_Dataloader
from generator import Generator
from discriminator import Discriminator
from rollout import ROLLOUT
from target_lstm import TARGET_LSTM

from utils.special_token_provisioner import SpecialTokenProvisioner
from utils.sequence_extractor import SequenceExtractor
from utils.dataset_reader import DatasetReader
from utils.vocab_dictionary import VocabDictionary

flags = tf.app.flags

# Logging and printout options
flags.DEFINE_integer("pretrain_g_epochs", 120, "number of epochs for pretrain the generator for, using MLE objective")
flags.DEFINE_integer("pretrain_d_epochs", 50, "number of epochs for pretrain the discriminator for")
flags.DEFINE_boolean("show_every_epoch", False, "show_every_epoch: print every epoch's stats")

# Model Architecture
### Generator
flags.DEFINE_integer("g_emb_dim", 32, "g_emb_dim: embedding size for generator")
flags.DEFINE_integer("g_hidden_dim", 32, "g_hidden_dim: hidden state size for generator lstm")
flags.DEFINE_float("g_learning_rate", 0.01, "g_learning_rate: learning rate for generator")
### Discriminator
flags.DEFINE_integer("d_emb_dim", 64, "d_emb_dim: embedding size for discriminator")
flags.DEFINE_float("d_learning_rate", 1e-4, "d_learning_rate: learning rate for discriminator")

### Both
flags.DEFINE_integer("batch_size", 64, "batch_size: batch size")
flags.DEFINE_integer("max_sequence_len", 20, "max_sequence_len: sequence length for oracle data, or length to pad to, for natural data")

# Adversarial training hyperparams
flags.DEFINE_integer("adversarial_epochs", 200, "number of adversarial training cycles")
flags.DEFINE_integer("g_steps", 1, "steps to train the generator in a given training cycle")
flags.DEFINE_integer("d_steps", 5, "number of times to generate data to train the discriminator in a given training cycle")
flags.DEFINE_integer("k_steps", 3, "epochs to train the discriminator on given set of generated data in a given training cycle")
flags.DEFINE_integer("rollout_sample_size", 16, "rollout_sample_size: number of times to run rollouts for any given prefix")

# Rollout network
flags.DEFINE_boolean("update_rollout_every_gstep", True, "update_rollout_every_gstep: update rollout network after every generator training step in a given training cycle")

# Dataset to train on
flags.DEFINE_boolean("use_oracle_data", True, "use_oracle_data: use an oracle to generate a synthetic dataset data. (Note: using oracle also means we have oracle NLL metric to evaluate quality.)")
flags.DEFINE_boolean("use_natural_data", False, "use_natural_data: use a real dataset, as opposed to a synthetic dataset generated by an oracle. (Note: using real data means there is no oracle NLL metric to evaluate quality.)")
flags.DEFINE_string("train_data_fp", "foo.csv", "train_data_fp: filepath to the real dataset (Note: flag ignored if using oracle)")
flags.DEFINE_string("generator_data_fp", "data", "generator_data_fp: filepath for a file the generator can write to (Note: flag ignored if using oracle)")
flags.DEFINE_string("eval_data_fp", "data", "eval_data_fp: filepath for a file the generator can write to (Note: flag ignored if using oracle)")
flags.DEFINE_boolean("use_character_level_model", False, "use_character_level_model: if True, model characters, not words (Note: flag ignored if using oracle)")
flags.DEFINE_boolean("use_onehot_embeddings", False, "use_onehot_embeddings: can only be used with use_character_level_model. Skips token embeddings.")

flags.DEFINE_string("train_data_colnames_commasep", "comment,label", "train_data_colnames_commasep: comma-separated list of column names in order")
flags.DEFINE_string("train_data_text_colname", "comment", "train_data_text_colname: column name containing the text we want to train on")
flags.DEFINE_string("train_data_sepchar", "\t", "train_data_sepchar: separator character for train dataset (if multiple columns)")


FLAGS = flags.FLAGS

#########################################################################################
#  Batch shape
######################################################################################
BATCH_SIZE = FLAGS.batch_size
SEQ_LENGTH = FLAGS.max_sequence_len # sequence length

#########################################################################################
#  Generator Hyper-parameters
######################################################################################
PRE_EPOCH_NUM = FLAGS.pretrain_g_epochs
WORD_EMB_DIM = FLAGS.g_emb_dim # embedding dimension
HIDDEN_DIM = FLAGS.g_hidden_dim # hidden state dimension of lstm cell

START_TOKEN = 0
UNK_TOKEN = 1
PAD_TOKEN = 2
EOS_TOKEN = 3

oracle_vocab_size = 5000  # if applicable

#########################################################################################
#  Discriminator Hyper-parameters
#########################################################################################
dis_pre_epoch_num = FLAGS.pretrain_d_epochs
dis_word_embedding_dim = FLAGS.d_emb_dim

dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]
dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]

dis_dropout_keep_prob = 0.75
dis_batch_size = 64

#########################################################################################
#  Adversarial Training Config
#########################################################################################
TOTAL_BATCH = FLAGS.adversarial_epochs
positive_file = 'save/real_data.txt' if FLAGS.use_oracle_data else FLAGS.train_data_fp
negative_file = 'save/generator_sample.txt' if FLAGS.use_oracle_data else FLAGS.generator_data_fp
eval_file = 'save/eval_file.txt' if FLAGS.use_oracle_data else FLAGS.eval_data_fp
generated_num = 10000

rollout_sample_size = FLAGS.rollout_sample_size
conditional_expectation_samples = 1 # the current implementation doesnt actually support > 1


os.makedirs('save', exist_ok=True)


def generate_samples(sess, trainable_model, batch_size, generated_num, output_file,
                     vocab_dict, seq_extractor):
    # Generate Samples
    generated_samples = []
    for _ in range(int(generated_num / batch_size)):
        sample_batch = trainable_model.generate(sess)
        generated_samples.extend(sample_batch)

    with open(output_file, 'w+') as fout:
        for sample in generated_samples:

            tokens = [vocab_dict.reverse_lookup(token_id) for token_id in sample]
            buffer = seq_extractor.join(tokens, drop_special_tokens=False)
            fout.write(buffer + '\n')
    fout.close()
    return

def compute_oracle_loss(sess, target_lstm, data_loader):
    # target_loss means the oracle negative log-likelihood tested with the oracle model "target_lstm"
    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473
    nll = []
    data_loader.reset_pointer()

    for it in range(data_loader.num_batch):
        x_batch = data_loader.next_batch()
        oracle_loss_batch = sess.run(target_lstm.pretrain_loss, {target_lstm.x: x_batch})
        nll.append(oracle_loss_batch)

    return np.mean(nll)

def compute_g_loss(sess, discriminator, data_loader):
    g_losses = []
    data_loader.reset_pointer()

    y_fake_batch = np.zeros((BATCH_SIZE, 2))
    y_fake_batch[:,1] += 1.0

    for it in range(data_loader.num_batch):
        x_fake_batch = data_loader.next_batch()

        feed = {discriminator.input_x: x_fake_batch,
                discriminator.input_y: y_fake_batch,
                discriminator.training_mode: False
                }
        g_loss_batch = sess.run(discriminator.loss, feed)
        g_losses.append(g_loss_batch)

    return np.mean(g_losses)

def compute_d_loss(sess, discriminator, data_loader):
    d_losses = []
    data_loader.reset_pointer()

    for it in range(data_loader.num_batch):
        x_mixed_batch, y_mixed_batch = data_loader.next_batch()

        feed = {discriminator.input_x: x_mixed_batch,
                discriminator.input_y: y_mixed_batch,
                discriminator.training_mode: False
                }
        d_loss_batch = sess.run(discriminator.loss, feed)
        d_losses.append(d_loss_batch)

    return np.mean(d_losses)

def mixeddata_label_prediction_avg(sess, discriminator, disc_data_loader):
    label_preds = []
    disc_data_loader.reset_pointer()

    for it in range(disc_data_loader.num_batch):
        x_batch, y_batch = disc_data_loader.next_batch()

        feed = {discriminator.input_x: x_batch,
                discriminator.training_mode: False
                }
        label_pred_batch = discriminator.predictions.eval(feed, session=sess)
        label_preds.append(label_pred_batch)

    return np.mean(label_preds)

def fakedata_label_prediction_avg(sess, discriminator, likelihood_data_loader):
    label_preds = []
    likelihood_data_loader.reset_pointer()

    for it in range(likelihood_data_loader.num_batch):
        x_batch = likelihood_data_loader.next_batch()

        feed = {discriminator.input_x: x_batch,
                discriminator.training_mode: False
                }
        label_pred_batch = discriminator.predictions.eval(feed, session=sess)
        label_preds.append(label_pred_batch)

    return np.mean(label_preds)

def realdata_label_prediction_avg(sess, discriminator, gen_data_loader):
    label_preds = []
    gen_data_loader.reset_pointer()

    for it in range(gen_data_loader.num_batch):
        x_batch = gen_data_loader.next_batch()

        feed = {discriminator.input_x: x_batch,
                discriminator.training_mode: False
                }
        label_pred_batch = discriminator.predictions.eval(feed, session=sess)
        label_preds.append(label_pred_batch)

    return np.mean(label_preds)


def pre_train_epoch(sess, trainable_model, data_loader):
    # Pretrains the generator for one epoch, using MLE on the token sequences
    pretrain_losses = []
    data_loader.reset_pointer()

    for it in range(data_loader.num_batch):
        batch = data_loader.next_batch()
        _, pretrain_loss_batch = trainable_model.pretrain_step(sess, batch)
        pretrain_losses.append(pretrain_loss_batch)

    return np.mean(pretrain_losses)

def log_all_the_things(sess, discriminator, mixed_data_loader, fake_data_loader, real_data_loader,
                       logging_prefix_string):

    kv = OrderedDict()

    d_loss = compute_d_loss(sess, discriminator, mixed_data_loader)
    kv['d_loss'] = d_loss

    g_loss = compute_g_loss(sess, discriminator, fake_data_loader)
    kv['g_loss'] = g_loss

    mixeddata_label_pred_avg = mixeddata_label_prediction_avg(sess, discriminator, mixed_data_loader)
    kv['mixeddata_label_pred_avg'] = mixeddata_label_pred_avg

    fakedata_label_pred_avg = fakedata_label_prediction_avg(sess, discriminator, fake_data_loader)
    kv['fakedata_label_pred_avg'] = fakedata_label_pred_avg

    realdata_label_pred_avg = realdata_label_prediction_avg(sess, discriminator, real_data_loader)
    kv['realdata_label_pred_avg'] = realdata_label_pred_avg

    timestamp = datetime.datetime.now()
    kv['timestamp'] = timestamp

    components = [logging_prefix_string]
    components.extend(["{}: {}".format(k, v) for k, v in kv.items()])

    log_msg = '\n\t '.join(components)
    print(log_msg)

    return kv


def main():
    assert START_TOKEN == 0
    assert ((FLAGS.use_oracle_data or FLAGS.use_natural_data) == True)

    assert ((FLAGS.use_character_level_model == True) if (FLAGS.use_onehot_embeddings == True)
            else (FLAGS.use_character_level_model == False))

    if FLAGS.use_natural_data:
        print("WARNING: " + \
              "since FLAGS.use_natural_data is set to True, " + \
              "we must are setting FLAGS.use_oracle_data to False.")
        FLAGS.use_oracle_data = False

    if FLAGS.use_oracle_data:
        print("WARNING: " + \
              "since FLAGS.use_oracle_data is set to True, " + \
              "we must are setting FLAGS.use_character_level_model to False.")
        FLAGS.use_character_level_model = False

    if FLAGS.use_oracle_data:
        tokenizer_level = 'oracletokens'
    else:
        if FLAGS.use_character_level_model:
            tokenizer_level = 'character'
        else:
            #tokenizer_level = 'word'
            tokenizer_level = 'socialmediatokens'

    special_tokens_provisioner = SpecialTokenProvisioner(tokenizer_level=tokenizer_level)
    special_tokens_specification = special_tokens_provisioner.special_tokens_specification

    seq_extractor = SequenceExtractor(tokenizer_level=tokenizer_level,
                                      special_tokens_spec=special_tokens_specification
    )


    if FLAGS.use_natural_data:
        real_dataset_reader = DatasetReader(fp=positive_file,
                                            file_sepchar=FLAGS.train_data_sepchar,
                                            file_columns=FLAGS.train_data_colnames_commasep.split(","),
                                            text_colname=FLAGS.train_data_text_colname,
                                            sequence_extractor=seq_extractor,
                                            min_seq_length=2,
                                            max_seq_length=SEQ_LENGTH,
                                            vocab_dict=None)
        real_dataset_reader.load()

        vocab_dict = real_dataset_reader.vocab_dict

        fake_dataset_reader = DatasetReader(fp=negative_file,
                                            file_sepchar="\t",
                                            file_columns=['comment'],
                                            text_colname='comment',
                                            sequence_extractor=seq_extractor,
                                            min_seq_length=2,
                                            max_seq_length=SEQ_LENGTH,
                                            vocab_dict=vocab_dict)

        vocab_size = vocab_dict.get_length()
    else:
        vocab_dict = VocabDictionary(seq_extractor=seq_extractor,
                                     max_seq_length=SEQ_LENGTH,
                                     drop_freq_thresh=0)
        vocab_dict.init_oracle(oracle_vocab_size)
        vocab_size = vocab_dict.get_length()

        real_dataset_reader = DatasetReader(fp=positive_file,
                                            file_sepchar="\t",
                                            file_columns=['comment'],
                                            text_colname='comment',
                                            sequence_extractor=seq_extractor,
                                            min_seq_length=2,
                                            max_seq_length=SEQ_LENGTH,
                                            vocab_dict=vocab_dict)

        fake_dataset_reader = DatasetReader(fp=negative_file,
                                            file_sepchar="\t",
                                            file_columns=['comment'],
                                            text_colname='comment',
                                            sequence_extractor=seq_extractor,
                                            min_seq_length=2,
                                            max_seq_length=SEQ_LENGTH,
                                            vocab_dict=vocab_dict)

    print(vocab_dict.vocab_dict)
    print(vocab_dict.int_to_token_dict)

    ######## Gen, Dis, and Oracle Models ###########
    EMB_DIM = WORD_EMB_DIM
    dis_embedding_dim = dis_word_embedding_dim

    if FLAGS.use_onehot_embeddings:
        # if we're using one-hot encodings,
        # the embedding dim must be the same as the number of possible tokens:
        EMB_DIM = vocab_size

    generator = Generator(
        vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH,
        go_token=START_TOKEN,
        eos_token=EOS_TOKEN,
        pad_token=(PAD_TOKEN if vocab_dict is not None else None),
        use_onehot_embeddings=FLAGS.use_onehot_embeddings,
        learning_rate=FLAGS.g_learning_rate
    )

    discriminator = Discriminator(
        sequence_length=SEQ_LENGTH,
        vocab_size=vocab_size,
        embedding_size=dis_embedding_dim,
        filter_sizes=dis_filter_sizes,
        num_filters=dis_num_filters,
        dropout_keep_prob=dis_dropout_keep_prob,
        learning_rate=FLAGS.d_learning_rate,
        pad_token=(PAD_TOKEN if vocab_dict is not None else None)
    )

    target_params = []
    target_lstm = TARGET_LSTM(
        vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH,
        START_TOKEN,
        target_params
    )

    ##################### Session Config #######################
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    sess = tf.Session(config=config)
    sess.run(tf.global_variables_initializer())

    ##################### Prepare the datasets #######################

    # Load the "real" data. If we're using an oracle, let the oracle synthesize it first.
    # Then load it via real_dataset_reader
    if FLAGS.use_oracle_data:
        generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file,
                         vocab_dict=vocab_dict, seq_extractor=seq_extractor
        )

    real_dataset_reader.load()

    # Load the "fake" data. This will look like nonsense with an untrained model.
    generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                     vocab_dict=vocab_dict, seq_extractor=seq_extractor
    )
    fake_dataset_reader.load()

    ##################### Instantiate the Dataloaders from the DataReaders and the VocabDict #######################

    # Data loaders -
    #
    # these the data extracted via the DataReaders
    # normalize the tokens (i.e., replace them with ints)
    # and create batches

    gen_data_loader = Gen_Dataloader(
        vocab_dict=vocab_dict,
        dataset_reader=real_dataset_reader,
        batch_size=BATCH_SIZE
    )

    likelihood_data_loader = Gen_Dataloader(
        vocab_dict=vocab_dict,
        dataset_reader=fake_dataset_reader,
        batch_size=BATCH_SIZE
    )

    dis_data_loader = Dis_Dataloader(
        vocab_dict=vocab_dict,
        positive_dataset_reader=real_dataset_reader,
        negative_dataset_reader=fake_dataset_reader,
        batch_size=BATCH_SIZE
    )

    gen_data_loader.create_batches()
    likelihood_data_loader.create_batches()
    dis_data_loader.load_train_data()

    log = open('save/experiment-log.txt', 'w+')

    print('Starting pre-training for the generator')
    log.write('pre-training...\n')

    for epoch in range(PRE_EPOCH_NUM):
        pretrain_cross_entropy_loss = pre_train_epoch(sess, generator, gen_data_loader)

        if epoch % 5 == 0 or FLAGS.show_every_epoch:

            if (FLAGS.use_natural_data == False):
                generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                                 vocab_dict=vocab_dict, seq_extractor=seq_extractor
                )

                likelihood_data_loader.create_batches()

                oracle_nll_loss = compute_oracle_loss(sess, target_lstm, likelihood_data_loader)

                print('generator pre-train epoch {}... oracle_nll {}... training set cross entropy loss {}... datetime {}'.format(
                        epoch, oracle_nll_loss, pretrain_cross_entropy_loss, datetime.datetime.now()
                ))
                buffer = 'epoch:\t' + str(epoch) + '\t' + \
                         'oracle_nll:\t' + str(oracle_nll_loss) + '\n'
                log.write(buffer)
            else:
                generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                                 vocab_dict=vocab_dict, seq_extractor=seq_extractor
                )

                gen_data_loader.create_batches()
                likelihood_data_loader.create_batches()
                dis_data_loader.load_train_data()

                logging_prefix_string = 'generator pre-train epoch {}\n\t token_cross_entropy_loss: {}'.format(
                    epoch, pretrain_cross_entropy_loss
                )

                log_all_the_things(
                    sess=sess, discriminator=discriminator,
                    mixed_data_loader=dis_data_loader,
                    fake_data_loader=likelihood_data_loader,
                    real_data_loader=gen_data_loader,
                    logging_prefix_string=logging_prefix_string
                )

                buffer = 'epoch:\t' + str(epoch) + '\t' + \
                         'pretrain_cross_entropy_loss:\t' + str(pretrain_cross_entropy_loss) + '\n'
                log.write(buffer)

    print('Starting pre-training for the discriminator...')
    for epoch in range(dis_pre_epoch_num):
        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                         vocab_dict=vocab_dict, seq_extractor=seq_extractor
        )

        dis_data_loader.load_train_data()

        for _ in range(FLAGS.k_steps):
            dis_data_loader.reset_pointer()
            for it in range(dis_data_loader.num_batch):
                x_batch, y_batch = dis_data_loader.next_batch()
                feed = {
                    discriminator.input_x: x_batch,
                    discriminator.input_y: y_batch,
                    discriminator.training_mode: True
                }
                _ = sess.run(discriminator.train_op, feed)

        if epoch % 5 == 0 or FLAGS.show_every_epoch:
            gen_data_loader.create_batches()
            likelihood_data_loader.create_batches()
            dis_data_loader.load_train_data()

            logging_prefix_string = 'discriminator pre-train epoch {}... '.format(epoch)

            log_all_the_things(
                sess=sess, discriminator=discriminator,
                mixed_data_loader=dis_data_loader,
                fake_data_loader=likelihood_data_loader,
                real_data_loader=gen_data_loader,
                logging_prefix_string=logging_prefix_string
            )

    rollout = ROLLOUT(generator, 0.80)

    print('#########################################################################')
    print('Start Adversarial Training...')
    log.write('adversarial training...\n')

    for total_batch in range(TOTAL_BATCH):
        # Train the generator for one step
        for it in range(FLAGS.g_steps):
            samples = generator.generate(sess)
            rewards = rollout.get_reward(sess, samples, rollout_sample_size, discriminator)
            feed = {generator.x: samples, generator.rewards: rewards}
            _ = sess.run(generator.g_updates, feed_dict=feed)

            # Update roll-out parameters
            if FLAGS.update_rollout_every_gstep:
                rollout.update_params()

        # Evaluate the generator
        if (total_batch % 5 == 0) or (total_batch == TOTAL_BATCH - 1) or FLAGS.show_every_epoch:

            if (FLAGS.use_natural_data == False):
                generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                                 vocab_dict=vocab_dict, seq_extractor=seq_extractor
                )

                likelihood_data_loader.create_batches()

                oracle_nll_loss = compute_oracle_loss(sess, target_lstm, likelihood_data_loader)

                print('epoch: {}\t generator training... oracle_nll: {}\t datetime: {}'.format(
                    total_batch, oracle_nll_loss, datetime.datetime.now()
                ))

                buffer = 'epoch:\t' + str(total_batch) + '\t' + \
                         'oracle_nll:\t' + str(oracle_nll_loss) + '\n'
                log.write(buffer)
            else:
                generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                                 vocab_dict=vocab_dict, seq_extractor=seq_extractor
                )

                gen_data_loader.create_batches()
                likelihood_data_loader.create_batches()
                dis_data_loader.load_train_data()

                logging_prefix_string = 'adversarial epoch: {}\n\t generator training... '.format(total_batch)

                kv = log_all_the_things(
                    sess=sess, discriminator=discriminator,
                    mixed_data_loader=dis_data_loader,
                    fake_data_loader=likelihood_data_loader,
                    real_data_loader=gen_data_loader,
                    logging_prefix_string=logging_prefix_string
                )

                g_loss = kv['g_loss']

                buffer = 'epoch:\t' + str(total_batch) + '\t' + \
                         'g_loss:\t' + str(g_loss) + '\n'
                log.write(buffer)

        # Update roll-out parameters, if we didn't already do so
        if not FLAGS.update_rollout_every_gstep:
            rollout.update_params()

        # Train the discriminator
        for _ in range(FLAGS.d_steps):
            generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file,
                             vocab_dict=vocab_dict, seq_extractor=seq_extractor
            )

            dis_data_loader.load_train_data()

            for _ in range(FLAGS.k_steps):
                dis_data_loader.reset_pointer()
                for it in range(dis_data_loader.num_batch):
                    x_batch, y_batch = dis_data_loader.next_batch()
                    feed = {
                        discriminator.input_x: x_batch,
                        discriminator.input_y: y_batch,
                        discriminator.training_mode: True
                    }
                    _ = sess.run(discriminator.train_op, feed)

        # Test
        if (total_batch % 5 == 0) or (total_batch == TOTAL_BATCH - 1) or FLAGS.show_every_epoch:

            gen_data_loader.create_batches()
            likelihood_data_loader.create_batches()
            dis_data_loader.load_train_data()

            logging_prefix_string = 'adversarial epoch: {}\n\t discriminator training... '.format(total_batch)

            kv = log_all_the_things(
                sess=sess, discriminator=discriminator,
                mixed_data_loader=dis_data_loader,
                fake_data_loader=likelihood_data_loader,
                real_data_loader=gen_data_loader,
                logging_prefix_string=logging_prefix_string
            )

            d_loss = kv['d_loss']

            buffer = 'epoch:\t' + str(total_batch) + '\t' + \
                     'd_loss:\t' + str(d_loss) + '\n'
            log.write(buffer)


    log.close()


if __name__ == '__main__':
    main()
